{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from pyspark.sql import SparkSession\n",
    "from insert_data import insert_fact, insert_dim_imm_per, insert_dim_imm_air, insert_dim_demo_info, insert_dim_demo_stat, insert_dim_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The purpose of this project is to see the connection between the temperature of the cities that people immigrate to. We could check the preferred destination cities for immigrants based on the immigration year as well. There will be 1 fact table with 5 dimension tables processed from the datasets below. This data model will be used for a further data analysis using SQL in order to see the relationship as I mentioned previously.\n",
    "\n",
    "__Data Used:__\n",
    "- I94 Immigration Data\n",
    "- Temperature Data\n",
    "- Demographics Data\n",
    "\n",
    "__Tools Used:__\n",
    "- AWS\n",
    "    - Redshift\n",
    "- Python\n",
    "    - PySpark\n",
    "- SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### I94 Immigration Data\n",
    "This data comes from [National Travel and Trourism Office(NTTO)](https://www.trade.gov/national-travel-and-tourism-office). The subject of the data is the immigrants going to the U.S. and the information is gives are where they come from, birth year, gender, visa type, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate',\n",
       "       'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'count',\n",
       "       'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd', 'entdepu',\n",
       "       'matflag', 'biryear', 'dtaddto', 'gender', 'insnum', 'airline',\n",
       "       'admnum', 'fltno', 'visatype'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cic_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>dep_city</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>dep_date</th>\n",
       "      <th>travel_code</th>\n",
       "      <th>visa</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cic_id    year  month dep_city  arrival_date  dep_date  travel_code  visa  \\\n",
       "0     6.0  2016.0    4.0      XXX       20573.0       NaN          NaN   2.0   \n",
       "1     7.0  2016.0    4.0      ATL       20551.0       NaN          1.0   3.0   \n",
       "2    15.0  2016.0    4.0      WAS       20545.0   20691.0          1.0   2.0   \n",
       "3    16.0  2016.0    4.0      NYC       20545.0   20567.0          1.0   2.0   \n",
       "4    17.0  2016.0    4.0      NYC       20545.0   20567.0          1.0   2.0   \n",
       "\n",
       "         country  \n",
       "0  United States  \n",
       "1  United States  \n",
       "2  United States  \n",
       "3  United States  \n",
       "4  United States  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_immigration = df[['cicid', 'i94yr', 'i94mon', 'i94port', 'arrdate', 'depdate', 'i94mode', 'i94visa']]\n",
    "fact_immigration.columns = ['cic_id', 'year', 'month', 'dep_city', 'arrival_date', 'dep_date', 'travel_code', 'visa']\n",
    "fact_immigration['country'] = 'United States'\n",
    "fact_immigration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cic_id</th>\n",
       "      <th>citizen_country</th>\n",
       "      <th>resident_country</th>\n",
       "      <th>birthyear</th>\n",
       "      <th>gender</th>\n",
       "      <th>ins_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cic_id  citizen_country  resident_country  birthyear gender ins_number\n",
       "0     6.0            692.0             692.0     1979.0    NaN        NaN\n",
       "1     7.0            254.0             276.0     1991.0      M        NaN\n",
       "2    15.0            101.0             101.0     1961.0      M        NaN\n",
       "3    16.0            101.0             101.0     1988.0    NaN        NaN\n",
       "4    17.0            101.0             101.0     2012.0    NaN        NaN"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_immigration_personal = df[['cicid', 'i94cit', 'i94res', 'biryear', 'gender', 'insnum']]\n",
    "dim_immigration_personal.columns = ['cic_id', 'citizen_country', 'resident_country', 'birthyear', 'gender', 'ins_number']\n",
    "dim_immigration_personal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cic_id</th>\n",
       "      <th>airline</th>\n",
       "      <th>admin_number</th>\n",
       "      <th>flight_number</th>\n",
       "      <th>visa</th>\n",
       "      <th>visa_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>3.0</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cic_id airline  admin_number flight_number  visa visa_type\n",
       "0     6.0     NaN  1.897628e+09           NaN   2.0        B2\n",
       "1     7.0     NaN  3.736796e+09         00296   3.0        F1\n",
       "2    15.0      OS  6.666432e+08            93   2.0        B2\n",
       "3    16.0      AA  9.246846e+10         00199   2.0        B2\n",
       "4    17.0      AA  9.246846e+10         00199   2.0        B2"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_immigration_air = df[['cicid', 'airline', 'admnum', 'fltno', 'i94visa', 'visatype']]\n",
    "dim_immigration_air.columns = ['cic_id', 'airline', 'admin_number', 'flight_number', 'visa', 'visa_type']\n",
    "dim_immigration_air.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/home/workspace/sas_data already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o47.parquet.\n: org.apache.spark.sql.AnalysisException: path file:/home/workspace/sas_data already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9814a8024ab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#write to parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sas_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_spark\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sas_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/home/workspace/sas_data already exists.;'"
     ]
    }
   ],
   "source": [
    "#write to parquet\n",
    "df_spark.write.parquet(\"sas_data\")\n",
    "df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature Data\n",
    "This data comes from [Kaggle](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data), and it shows the temperature in different cities around the world. It's recorded monthly, and the values are the average temperature of that month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temp = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Ã…rhus   \n",
       "1  1743-12-01                 NaN                            NaN  Ã…rhus   \n",
       "2  1744-01-01                 NaN                            NaN  Ã…rhus   \n",
       "3  1744-02-01                 NaN                            NaN  Ã…rhus   \n",
       "4  1744-03-01                 NaN                            NaN  Ã…rhus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>avg_temp</th>\n",
       "      <th>avg_temp_uncertainty</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47555</th>\n",
       "      <td>1820-01-01</td>\n",
       "      <td>2.101</td>\n",
       "      <td>3.217</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47556</th>\n",
       "      <td>1820-02-01</td>\n",
       "      <td>6.926</td>\n",
       "      <td>2.853</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47557</th>\n",
       "      <td>1820-03-01</td>\n",
       "      <td>10.767</td>\n",
       "      <td>2.395</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47558</th>\n",
       "      <td>1820-04-01</td>\n",
       "      <td>17.989</td>\n",
       "      <td>2.202</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47559</th>\n",
       "      <td>1820-05-01</td>\n",
       "      <td>21.809</td>\n",
       "      <td>2.036</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dt  avg_temp  avg_temp_uncertainty     city        country  \\\n",
       "47555  1820-01-01     2.101                 3.217  Abilene  United States   \n",
       "47556  1820-02-01     6.926                 2.853  Abilene  United States   \n",
       "47557  1820-03-01    10.767                 2.395  Abilene  United States   \n",
       "47558  1820-04-01    17.989                 2.202  Abilene  United States   \n",
       "47559  1820-05-01    21.809                 2.036  Abilene  United States   \n",
       "\n",
       "      latitude longitude  \n",
       "47555   32.95N   100.53W  \n",
       "47556   32.95N   100.53W  \n",
       "47557   32.95N   100.53W  \n",
       "47558   32.95N   100.53W  \n",
       "47559   32.95N   100.53W  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_us = df_temp[df_temp['Country'] == 'United States']\n",
    "df_temp_us.columns = ['dt', 'avg_temp', 'avg_temp_uncertainty', 'city', 'country', 'latitude', 'longitude']\n",
    "df_temp_us.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>avg_temp</th>\n",
       "      <th>avg_temp_uncertainty</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47555</th>\n",
       "      <td>1820-01-01</td>\n",
       "      <td>2.101</td>\n",
       "      <td>3.217</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "      <td>1</td>\n",
       "      <td>1820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47556</th>\n",
       "      <td>1820-02-01</td>\n",
       "      <td>6.926</td>\n",
       "      <td>2.853</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "      <td>2</td>\n",
       "      <td>1820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47557</th>\n",
       "      <td>1820-03-01</td>\n",
       "      <td>10.767</td>\n",
       "      <td>2.395</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "      <td>3</td>\n",
       "      <td>1820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47558</th>\n",
       "      <td>1820-04-01</td>\n",
       "      <td>17.989</td>\n",
       "      <td>2.202</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "      <td>4</td>\n",
       "      <td>1820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47559</th>\n",
       "      <td>1820-05-01</td>\n",
       "      <td>21.809</td>\n",
       "      <td>2.036</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "      <td>5</td>\n",
       "      <td>1820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dt  avg_temp  avg_temp_uncertainty     city        country  \\\n",
       "47555 1820-01-01     2.101                 3.217  Abilene  United States   \n",
       "47556 1820-02-01     6.926                 2.853  Abilene  United States   \n",
       "47557 1820-03-01    10.767                 2.395  Abilene  United States   \n",
       "47558 1820-04-01    17.989                 2.202  Abilene  United States   \n",
       "47559 1820-05-01    21.809                 2.036  Abilene  United States   \n",
       "\n",
       "      latitude longitude  month  year  \n",
       "47555   32.95N   100.53W      1  1820  \n",
       "47556   32.95N   100.53W      2  1820  \n",
       "47557   32.95N   100.53W      3  1820  \n",
       "47558   32.95N   100.53W      4  1820  \n",
       "47559   32.95N   100.53W      5  1820  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_us['dt'] = pd.to_datetime(df_temp_us['dt'])\n",
    "df_temp_us['month'] = pd.DatetimeIndex(df_temp_us['dt']).month\n",
    "df_temp_us['year'] = pd.DatetimeIndex(df_temp_us['dt']).year\n",
    "df_temp_us.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### US Cities Demographics Data\n",
    "This data comes from [OpenSoft](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/), and it shows the demographics of the cities in the U.S. including the population, median age, the state the city belongs to, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname = './us-cities-demographics.csv'\n",
    "df_demo = pd.read_csv(fname, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>m_population</th>\n",
       "      <th>f_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>num_of_veterans</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>state_code</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               city          state  m_population  f_population  \\\n",
       "0     Silver Spring       Maryland       40601.0       41862.0   \n",
       "1            Quincy  Massachusetts       44129.0       49500.0   \n",
       "2            Hoover        Alabama       38040.0       46799.0   \n",
       "3  Rancho Cucamonga     California       88127.0       87105.0   \n",
       "4            Newark     New Jersey      138040.0      143873.0   \n",
       "\n",
       "   total_population  num_of_veterans  foreign_born state_code  \\\n",
       "0             82463           1562.0       30908.0         MD   \n",
       "1             93629           4147.0       32935.0         MA   \n",
       "2             84839           4819.0        8229.0         AL   \n",
       "3            175232           5821.0       33878.0         CA   \n",
       "4            281913           5829.0       86253.0         NJ   \n",
       "\n",
       "                        race  \n",
       "0         Hispanic or Latino  \n",
       "1                      White  \n",
       "2                      Asian  \n",
       "3  Black or African-American  \n",
       "4                      White  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo_info = df_demo[['City', 'State', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born', 'State Code', 'Race']]\n",
    "df_demo_info.columns = ['city', 'state', 'm_population', 'f_population', 'total_population', 'num_of_veterans', 'foreign_born', 'state_code', 'race']\n",
    "df_demo_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>avg_household_size</th>\n",
       "      <th>state_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               city          state  median_age  avg_household_size state_code\n",
       "0     Silver Spring       Maryland        33.8                2.60         MD\n",
       "1            Quincy  Massachusetts        41.0                2.39         MA\n",
       "2            Hoover        Alabama        38.5                2.58         AL\n",
       "3  Rancho Cucamonga     California        34.5                3.18         CA\n",
       "4            Newark     New Jersey        34.6                2.73         NJ"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo_stat = df_demo[['City', 'State', 'Median Age', 'Average Household Size', 'State Code']]\n",
    "df_demo_stat.columns = ['city', 'state', 'median_age', 'avg_household_size', 'state_code']\n",
    "df_demo_stat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cic_id          float64\n",
       "year            float64\n",
       "month           float64\n",
       "dep_city         object\n",
       "arrival_date    float64\n",
       "dep_date        float64\n",
       "travel_code     float64\n",
       "visa            float64\n",
       "country          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_immigration.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Converting floats into datetime format for fact_immigration table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cic_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>dep_city</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>dep_date</th>\n",
       "      <th>travel_code</th>\n",
       "      <th>visa</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2016-04-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-08-25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cic_id    year  month dep_city arrival_date   dep_date  travel_code  visa  \\\n",
       "0     6.0  2016.0    4.0      XXX   2016-04-29        NaT          NaN   2.0   \n",
       "1     7.0  2016.0    4.0      ATL   2016-04-07        NaT          1.0   3.0   \n",
       "2    15.0  2016.0    4.0      WAS   2016-04-01 2016-08-25          1.0   2.0   \n",
       "3    16.0  2016.0    4.0      NYC   2016-04-01 2016-04-23          1.0   2.0   \n",
       "4    17.0  2016.0    4.0      NYC   2016-04-01 2016-04-23          1.0   2.0   \n",
       "\n",
       "         country  \n",
       "0  United States  \n",
       "1  United States  \n",
       "2  United States  \n",
       "3  United States  \n",
       "4  United States  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_immigration['arrival_date'] = pd.to_datetime(fact_immigration['arrival_date'], unit='D',\n",
    "               origin=pd.Timestamp('1960-01-01'))\n",
    "fact_immigration['dep_date'] = pd.to_datetime(fact_immigration['dep_date'], unit='D',\n",
    "               origin=pd.Timestamp('1960-01-01'))\n",
    "fact_immigration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_immigration = fact_immigration[pd.notnull(fact_immigration['arrival_date'])]\n",
    "fact_immigration = fact_immigration[pd.notnull(fact_immigration['dep_date'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Dropping the non-integer rows in flight_number column in the dim_immigration_air table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cic_id           float64\n",
       "airline           object\n",
       "admin_number     float64\n",
       "flight_number     object\n",
       "visa             float64\n",
       "visa_type         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_immigration_air.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cic_id</th>\n",
       "      <th>airline</th>\n",
       "      <th>admin_number</th>\n",
       "      <th>flight_number</th>\n",
       "      <th>visa</th>\n",
       "      <th>visa_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096283</th>\n",
       "      <td>5200122.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.466694e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096284</th>\n",
       "      <td>5200121.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.466708e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096285</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096286</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096287</th>\n",
       "      <td>2303672.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.870831e+08</td>\n",
       "      <td>LAND</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096288</th>\n",
       "      <td>625288.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.266689e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096289</th>\n",
       "      <td>2873230.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.806588e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096290</th>\n",
       "      <td>2873231.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.806625e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096291</th>\n",
       "      <td>2873232.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.806640e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096292</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096293</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096294</th>\n",
       "      <td>218224.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.251075e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096295</th>\n",
       "      <td>2873233.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.356363e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096296</th>\n",
       "      <td>2873234.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.356366e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096297</th>\n",
       "      <td>3298796.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.379549e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096298</th>\n",
       "      <td>4249463.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.428408e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096299</th>\n",
       "      <td>1153287.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.878366e+08</td>\n",
       "      <td>LAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096300</th>\n",
       "      <td>1153288.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.880218e+08</td>\n",
       "      <td>LAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096301</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096302</th>\n",
       "      <td>4471818.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.429612e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096303</th>\n",
       "      <td>4471817.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.429629e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096304</th>\n",
       "      <td>4471819.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.429643e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096305</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096306</th>\n",
       "      <td>4249464.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.426909e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096307</th>\n",
       "      <td>5416391.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.471480e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096308</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096309</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096310</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096311</th>\n",
       "      <td>5658953.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.488710e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096312</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3096313 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cic_id airline  admin_number flight_number  visa visa_type\n",
       "0              NaN     NaN           NaN           NaN   NaN       NaN\n",
       "1              NaN     NaN           NaN           NaN   NaN       NaN\n",
       "2              NaN     NaN           NaN           NaN   NaN       NaN\n",
       "3              NaN     NaN           NaN           NaN   NaN       NaN\n",
       "4              NaN     NaN           NaN           NaN   NaN       NaN\n",
       "5              NaN     NaN           NaN           NaN   NaN       NaN\n",
       "6              NaN     NaN           NaN           NaN   NaN       NaN\n",
       "7              NaN     NaN           NaN           NaN   NaN       NaN\n",
       "8              NaN     NaN           NaN           NaN   NaN       NaN\n",
       "9              NaN     NaN           NaN           NaN   NaN       NaN\n",
       "10             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "11             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "12             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "13             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "14             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "15             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "16             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "17             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "18             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "19             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "20             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "21             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "22             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "23             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "24             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "25             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "26             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "27             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "28             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "29             NaN     NaN           NaN           NaN   NaN       NaN\n",
       "...            ...     ...           ...           ...   ...       ...\n",
       "3096283  5200122.0     NaN  9.466694e+10          LAND   2.0        B2\n",
       "3096284  5200121.0     NaN  9.466708e+10          LAND   2.0        B2\n",
       "3096285        NaN     NaN           NaN           NaN   NaN       NaN\n",
       "3096286        NaN     NaN           NaN           NaN   NaN       NaN\n",
       "3096287  2303672.0     NaN  7.870831e+08          LAND   1.0        B1\n",
       "3096288   625288.0     NaN  9.266689e+10          LAND   1.0        B1\n",
       "3096289  2873230.0     NaN  8.806588e+10          LAND   2.0        B2\n",
       "3096290  2873231.0     NaN  8.806625e+10          LAND   2.0        B2\n",
       "3096291  2873232.0     NaN  8.806640e+10          LAND   2.0        B2\n",
       "3096292        NaN     NaN           NaN           NaN   NaN       NaN\n",
       "3096293        NaN     NaN           NaN           NaN   NaN       NaN\n",
       "3096294   218224.0     NaN  9.251075e+10          LAND   2.0        B2\n",
       "3096295  2873233.0     NaN  9.356363e+10          LAND   2.0        B2\n",
       "3096296  2873234.0     NaN  9.356366e+10          LAND   2.0        B2\n",
       "3096297  3298796.0     NaN  9.379549e+10          LAND   1.0        B1\n",
       "3096298  4249463.0     NaN  9.428408e+10          LAND   1.0        B1\n",
       "3096299  1153287.0     NaN  6.878366e+08          LAND   2.0        B2\n",
       "3096300  1153288.0     NaN  6.880218e+08          LAND   2.0        B2\n",
       "3096301        NaN     NaN           NaN           NaN   NaN       NaN\n",
       "3096302  4471818.0     NaN  9.429612e+10          LAND   2.0        B2\n",
       "3096303  4471817.0     NaN  9.429629e+10          LAND   2.0        B2\n",
       "3096304  4471819.0     NaN  9.429643e+10          LAND   2.0        B2\n",
       "3096305        NaN     NaN           NaN           NaN   NaN       NaN\n",
       "3096306  4249464.0     NaN  9.426909e+10          LAND   1.0        B1\n",
       "3096307  5416391.0     NaN  9.471480e+10          LAND   1.0        B1\n",
       "3096308        NaN     NaN           NaN           NaN   NaN       NaN\n",
       "3096309        NaN     NaN           NaN           NaN   NaN       NaN\n",
       "3096310        NaN     NaN           NaN           NaN   NaN       NaN\n",
       "3096311  5658953.0     NaN  9.488710e+10          LAND   2.0        B2\n",
       "3096312        NaN     NaN           NaN           NaN   NaN       NaN\n",
       "\n",
       "[3096313 rows x 6 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_immigration_air.where(dim_immigration_air['flight_number'] == 'LAND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cic_id                   610\n",
       "airline                  *GA\n",
       "admin_number     9.24611e+10\n",
       "flight_number          VPCSW\n",
       "visa                       2\n",
       "visa_type                 B2\n",
       "Name: 540, dtype: object"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_immigration_air.iloc[540]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dim_immigration_air['flight_number'] = dim_immigration_air[dim_immigration_air['flight_number'].apply(lambda x: str(x).isdigit())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dim_immigration_air['flight_number'] = pd.to_numeric(dim_immigration_air['flight_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cic_id           float64\n",
       "airline           object\n",
       "admin_number     float64\n",
       "flight_number    float64\n",
       "visa             float64\n",
       "visa_type         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_immigration_air.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Extracting necessary information from i94 description file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"I94_SAS_Labels_Descriptions.SAS\") as f:\n",
    "    contents = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "countries = contents[9:298]\n",
    "ports = contents[302:962]\n",
    "modes = contents[972:976]\n",
    "states = contents[981:1036]\n",
    "visa = contents[1046:1049]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>country_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>582</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>236</td>\n",
       "      <td>AFGHANISTAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>316</td>\n",
       "      <td>ALGERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>ANDORRA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_code                                       country_name\n",
       "0         582     MEXICO Air Sea, and Not Reported (I-94, no l...\n",
       "1         236                                         AFGHANISTAN\n",
       "2         101                                             ALBANIA\n",
       "3         316                                             ALGERIA\n",
       "4         102                                             ANDORRA"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = [x.strip().split('=') for x in countries]\n",
    "country_code = [x[0].replace(\"'\",\"\") for x in country]\n",
    "country_name = [x[1].replace(\"'\",\"\") for x in country]\n",
    "df_country = pd.DataFrame({'country_code':country_code, 'country_name':country_name})\n",
    "df_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>port_code</th>\n",
       "      <th>port_loc_city</th>\n",
       "      <th>port_loc_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALC</td>\n",
       "      <td>ALCAN</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANC</td>\n",
       "      <td>ANCHORAGE</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAR</td>\n",
       "      <td>BAKER AAF - BAKER ISLAND</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DAC</td>\n",
       "      <td>DALTONS CACHE</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIZ</td>\n",
       "      <td>DEW STATION PT LAY DEW</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  port_code             port_loc_city    port_loc_state\n",
       "0       ALC                     ALCAN   AK             \n",
       "1       ANC                 ANCHORAGE       AK         \n",
       "2       BAR  BAKER AAF - BAKER ISLAND                AK\n",
       "3       DAC             DALTONS CACHE           AK     \n",
       "4       PIZ    DEW STATION PT LAY DEW                AK"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port = [x.strip().split('=') for x in ports]\n",
    "port_code = [x[0].replace(\"'\",\"\").strip('\\t') for x in port]\n",
    "port_loc = [x[1].replace(\"'\",\"\").strip('\\t') for x in port]\n",
    "port_loc_c = [x.split(',')[0] for x in port_loc]\n",
    "port_loc_s = [x.split(',')[-1] for x in port_loc]\n",
    "df_port = pd.DataFrame({'port_code':port_code, 'port_loc_city':port_loc_c, 'port_loc_state':port_loc_s})\n",
    "df_port.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Not reported</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code            mode\n",
       "0   1              Air\n",
       "1   2              Sea\n",
       "2   3             Land\n",
       "3   9    Not reported "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode = [x.strip().split('=') for x in modes]\n",
    "code = [x[0] for x in mode]\n",
    "mode_name = [x[1].replace(\"'\", \"\").strip(\";\") for x in mode]\n",
    "df_mode = pd.DataFrame({'code':code, 'mode':mode_name})\n",
    "df_mode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_code</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>ALABAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>ARIZONA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>ARKANSAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state_code       state\n",
       "0         AL     ALABAMA\n",
       "1         AK      ALASKA\n",
       "2         AZ     ARIZONA\n",
       "3         AR    ARKANSAS\n",
       "4         CA  CALIFORNIA"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = [x.strip().split('=') for x in states]\n",
    "code = [x[0].replace(\"'\", \"\") for x in state]\n",
    "state_name = [x[1].replace(\"'\", \"\") for x in state]\n",
    "df_state = pd.DataFrame({'state_code':code, 'state':state_name})\n",
    "df_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>visa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Student</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code       visa\n",
       "0   1    Business\n",
       "1   2    Pleasure\n",
       "2   3     Student"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visa_list = [x.strip().split('=') for x in visa]\n",
    "code = [x[0] for x in visa_list]\n",
    "visa_name = [x[1] for x in visa_list]\n",
    "df_visa = pd.DataFrame({'code':code, 'visa':visa_name})\n",
    "df_visa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2324"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo_info.duplicated(subset=['city']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2324"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo_stat.duplicated(subset=['city']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~df_demo_info.duplicated()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~df_demo_stat.duplicated()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>m_population</th>\n",
       "      <th>f_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>num_of_veterans</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>state_code</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Peoria</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>56229.0</td>\n",
       "      <td>62432.0</td>\n",
       "      <td>118661</td>\n",
       "      <td>6634.0</td>\n",
       "      <td>7517.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avondale</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>38712.0</td>\n",
       "      <td>41971.0</td>\n",
       "      <td>80683</td>\n",
       "      <td>4815.0</td>\n",
       "      <td>8355.0</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Black or African-American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>West Covina</td>\n",
       "      <td>California</td>\n",
       "      <td>51629.0</td>\n",
       "      <td>56860.0</td>\n",
       "      <td>108489</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>37038.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O'Fallon</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>41762.0</td>\n",
       "      <td>43270.0</td>\n",
       "      <td>85032</td>\n",
       "      <td>5783.0</td>\n",
       "      <td>3269.0</td>\n",
       "      <td>MO</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High Point</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>51751.0</td>\n",
       "      <td>58077.0</td>\n",
       "      <td>109828</td>\n",
       "      <td>5204.0</td>\n",
       "      <td>16315.0</td>\n",
       "      <td>NC</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Folsom</td>\n",
       "      <td>California</td>\n",
       "      <td>41051.0</td>\n",
       "      <td>35317.0</td>\n",
       "      <td>76368</td>\n",
       "      <td>4187.0</td>\n",
       "      <td>13234.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Folsom</td>\n",
       "      <td>California</td>\n",
       "      <td>41051.0</td>\n",
       "      <td>35317.0</td>\n",
       "      <td>76368</td>\n",
       "      <td>4187.0</td>\n",
       "      <td>13234.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>741270.0</td>\n",
       "      <td>826172.0</td>\n",
       "      <td>1567442</td>\n",
       "      <td>61995.0</td>\n",
       "      <td>205339.0</td>\n",
       "      <td>PA</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Wichita</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>192354.0</td>\n",
       "      <td>197601.0</td>\n",
       "      <td>389955</td>\n",
       "      <td>23978.0</td>\n",
       "      <td>40270.0</td>\n",
       "      <td>KS</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Wichita</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>192354.0</td>\n",
       "      <td>197601.0</td>\n",
       "      <td>389955</td>\n",
       "      <td>23978.0</td>\n",
       "      <td>40270.0</td>\n",
       "      <td>KS</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Fort Myers</td>\n",
       "      <td>Florida</td>\n",
       "      <td>36850.0</td>\n",
       "      <td>37165.0</td>\n",
       "      <td>74015</td>\n",
       "      <td>4312.0</td>\n",
       "      <td>15365.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>149690.0</td>\n",
       "      <td>154695.0</td>\n",
       "      <td>304385</td>\n",
       "      <td>17728.0</td>\n",
       "      <td>28187.0</td>\n",
       "      <td>PA</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Laredo</td>\n",
       "      <td>Texas</td>\n",
       "      <td>124305.0</td>\n",
       "      <td>131484.0</td>\n",
       "      <td>255789</td>\n",
       "      <td>4921.0</td>\n",
       "      <td>68427.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Berkeley</td>\n",
       "      <td>California</td>\n",
       "      <td>60142.0</td>\n",
       "      <td>60829.0</td>\n",
       "      <td>120971</td>\n",
       "      <td>3736.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>California</td>\n",
       "      <td>63278.0</td>\n",
       "      <td>62938.0</td>\n",
       "      <td>126216</td>\n",
       "      <td>4426.0</td>\n",
       "      <td>52281.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Allen</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>60626.0</td>\n",
       "      <td>59581.0</td>\n",
       "      <td>120207</td>\n",
       "      <td>5691.0</td>\n",
       "      <td>19652.0</td>\n",
       "      <td>PA</td>\n",
       "      <td>Black or African-American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Hampton</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>66214.0</td>\n",
       "      <td>70240.0</td>\n",
       "      <td>136454</td>\n",
       "      <td>19638.0</td>\n",
       "      <td>6204.0</td>\n",
       "      <td>VA</td>\n",
       "      <td>Black or African-American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Bolingbrook</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>36295.0</td>\n",
       "      <td>35801.0</td>\n",
       "      <td>72096</td>\n",
       "      <td>2951.0</td>\n",
       "      <td>15212.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Frederick</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33146.0</td>\n",
       "      <td>36336.0</td>\n",
       "      <td>69482</td>\n",
       "      <td>3870.0</td>\n",
       "      <td>14211.0</td>\n",
       "      <td>MD</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sparks</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>47780.0</td>\n",
       "      <td>48318.0</td>\n",
       "      <td>96098</td>\n",
       "      <td>7315.0</td>\n",
       "      <td>15690.0</td>\n",
       "      <td>NV</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Rancho Cordova</td>\n",
       "      <td>California</td>\n",
       "      <td>34844.0</td>\n",
       "      <td>36182.0</td>\n",
       "      <td>71026</td>\n",
       "      <td>4590.0</td>\n",
       "      <td>17020.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Westminster</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>54866.0</td>\n",
       "      <td>58251.0</td>\n",
       "      <td>113117</td>\n",
       "      <td>6512.0</td>\n",
       "      <td>11361.0</td>\n",
       "      <td>CO</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Lakewood</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>76013.0</td>\n",
       "      <td>76576.0</td>\n",
       "      <td>152589</td>\n",
       "      <td>9988.0</td>\n",
       "      <td>14169.0</td>\n",
       "      <td>CO</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Flint</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>48984.0</td>\n",
       "      <td>49313.0</td>\n",
       "      <td>98297</td>\n",
       "      <td>3757.0</td>\n",
       "      <td>2138.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>New Haven</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>63765.0</td>\n",
       "      <td>66545.0</td>\n",
       "      <td>130310</td>\n",
       "      <td>2567.0</td>\n",
       "      <td>25871.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>Scranton</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>37975.0</td>\n",
       "      <td>39137.0</td>\n",
       "      <td>77112</td>\n",
       "      <td>5059.0</td>\n",
       "      <td>8069.0</td>\n",
       "      <td>PA</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862</th>\n",
       "      <td>Aurora</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>177899.0</td>\n",
       "      <td>180971.0</td>\n",
       "      <td>358870</td>\n",
       "      <td>25158.0</td>\n",
       "      <td>65816.0</td>\n",
       "      <td>CO</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863</th>\n",
       "      <td>Redlands</td>\n",
       "      <td>California</td>\n",
       "      <td>33993.0</td>\n",
       "      <td>37035.0</td>\n",
       "      <td>71028</td>\n",
       "      <td>3445.0</td>\n",
       "      <td>8681.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>Alhambra</td>\n",
       "      <td>California</td>\n",
       "      <td>42184.0</td>\n",
       "      <td>43388.0</td>\n",
       "      <td>85572</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>44441.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2865</th>\n",
       "      <td>Alhambra</td>\n",
       "      <td>California</td>\n",
       "      <td>42184.0</td>\n",
       "      <td>43388.0</td>\n",
       "      <td>85572</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>44441.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2866</th>\n",
       "      <td>Rio Rancho</td>\n",
       "      <td>New Mexico</td>\n",
       "      <td>47251.0</td>\n",
       "      <td>46904.0</td>\n",
       "      <td>94155</td>\n",
       "      <td>8527.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>NM</td>\n",
       "      <td>Black or African-American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2867</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>35967.0</td>\n",
       "      <td>39310.0</td>\n",
       "      <td>75277</td>\n",
       "      <td>3447.0</td>\n",
       "      <td>8612.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>Reading</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>43668.0</td>\n",
       "      <td>44205.0</td>\n",
       "      <td>87873</td>\n",
       "      <td>2580.0</td>\n",
       "      <td>17043.0</td>\n",
       "      <td>PA</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>Rialto</td>\n",
       "      <td>California</td>\n",
       "      <td>49902.0</td>\n",
       "      <td>53235.0</td>\n",
       "      <td>103137</td>\n",
       "      <td>2036.0</td>\n",
       "      <td>32741.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2870</th>\n",
       "      <td>Tallahassee</td>\n",
       "      <td>Florida</td>\n",
       "      <td>89390.0</td>\n",
       "      <td>100504.0</td>\n",
       "      <td>189894</td>\n",
       "      <td>9575.0</td>\n",
       "      <td>16720.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2871</th>\n",
       "      <td>Hemet</td>\n",
       "      <td>California</td>\n",
       "      <td>38622.0</td>\n",
       "      <td>45251.0</td>\n",
       "      <td>83873</td>\n",
       "      <td>5613.0</td>\n",
       "      <td>13330.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>Aurora</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>101964.0</td>\n",
       "      <td>101751.0</td>\n",
       "      <td>203715</td>\n",
       "      <td>4988.0</td>\n",
       "      <td>50980.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>Black or African-American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>Carmel</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>42928.0</td>\n",
       "      <td>46101.0</td>\n",
       "      <td>89029</td>\n",
       "      <td>2723.0</td>\n",
       "      <td>9645.0</td>\n",
       "      <td>IN</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>Glendale</td>\n",
       "      <td>California</td>\n",
       "      <td>98181.0</td>\n",
       "      <td>102844.0</td>\n",
       "      <td>201025</td>\n",
       "      <td>4448.0</td>\n",
       "      <td>111510.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>Des Moines</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>103726.0</td>\n",
       "      <td>106591.0</td>\n",
       "      <td>210317</td>\n",
       "      <td>11780.0</td>\n",
       "      <td>23857.0</td>\n",
       "      <td>IA</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>College Station</td>\n",
       "      <td>Texas</td>\n",
       "      <td>54125.0</td>\n",
       "      <td>53774.0</td>\n",
       "      <td>107899</td>\n",
       "      <td>2471.0</td>\n",
       "      <td>16145.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>Peoria</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>80139.0</td>\n",
       "      <td>91103.0</td>\n",
       "      <td>171242</td>\n",
       "      <td>13019.0</td>\n",
       "      <td>14260.0</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>Charlotte</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>396646.0</td>\n",
       "      <td>430475.0</td>\n",
       "      <td>827121</td>\n",
       "      <td>36046.0</td>\n",
       "      <td>128897.0</td>\n",
       "      <td>NC</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>Lafayette</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>34313.0</td>\n",
       "      <td>36857.0</td>\n",
       "      <td>71170</td>\n",
       "      <td>5045.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>IN</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>Abilene</td>\n",
       "      <td>Texas</td>\n",
       "      <td>65212.0</td>\n",
       "      <td>60664.0</td>\n",
       "      <td>125876</td>\n",
       "      <td>9367.0</td>\n",
       "      <td>8129.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>Black or African-American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2881</th>\n",
       "      <td>Gulfport</td>\n",
       "      <td>Mississippi</td>\n",
       "      <td>33108.0</td>\n",
       "      <td>38764.0</td>\n",
       "      <td>71872</td>\n",
       "      <td>6646.0</td>\n",
       "      <td>3072.0</td>\n",
       "      <td>MS</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2882</th>\n",
       "      <td>Davis</td>\n",
       "      <td>California</td>\n",
       "      <td>33493.0</td>\n",
       "      <td>34163.0</td>\n",
       "      <td>67656</td>\n",
       "      <td>2176.0</td>\n",
       "      <td>13997.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2883</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>1958998.0</td>\n",
       "      <td>2012898.0</td>\n",
       "      <td>3971896</td>\n",
       "      <td>85417.0</td>\n",
       "      <td>1485425.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884</th>\n",
       "      <td>Mount Vernon</td>\n",
       "      <td>New York</td>\n",
       "      <td>31876.0</td>\n",
       "      <td>36745.0</td>\n",
       "      <td>68621</td>\n",
       "      <td>2064.0</td>\n",
       "      <td>23777.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>Lynchburg</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>38614.0</td>\n",
       "      <td>41198.0</td>\n",
       "      <td>79812</td>\n",
       "      <td>4322.0</td>\n",
       "      <td>4364.0</td>\n",
       "      <td>VA</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886</th>\n",
       "      <td>Stockton</td>\n",
       "      <td>California</td>\n",
       "      <td>150976.0</td>\n",
       "      <td>154674.0</td>\n",
       "      <td>305650</td>\n",
       "      <td>12822.0</td>\n",
       "      <td>79583.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>Southfield</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>31369.0</td>\n",
       "      <td>41808.0</td>\n",
       "      <td>73177</td>\n",
       "      <td>4035.0</td>\n",
       "      <td>4011.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>410615.0</td>\n",
       "      <td>437808.0</td>\n",
       "      <td>848423</td>\n",
       "      <td>42186.0</td>\n",
       "      <td>72456.0</td>\n",
       "      <td>IN</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>Somerville</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41028.0</td>\n",
       "      <td>39306.0</td>\n",
       "      <td>80334</td>\n",
       "      <td>2103.0</td>\n",
       "      <td>22292.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>Coral Springs</td>\n",
       "      <td>Florida</td>\n",
       "      <td>63316.0</td>\n",
       "      <td>66186.0</td>\n",
       "      <td>129502</td>\n",
       "      <td>4724.0</td>\n",
       "      <td>38552.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2889 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  city           state  m_population  f_population  \\\n",
       "0        Silver Spring        Maryland       40601.0       41862.0   \n",
       "1               Quincy   Massachusetts       44129.0       49500.0   \n",
       "2               Hoover         Alabama       38040.0       46799.0   \n",
       "3     Rancho Cucamonga      California       88127.0       87105.0   \n",
       "4               Newark      New Jersey      138040.0      143873.0   \n",
       "5               Peoria        Illinois       56229.0       62432.0   \n",
       "6             Avondale         Arizona       38712.0       41971.0   \n",
       "7          West Covina      California       51629.0       56860.0   \n",
       "8             O'Fallon        Missouri       41762.0       43270.0   \n",
       "9           High Point  North Carolina       51751.0       58077.0   \n",
       "10              Folsom      California       41051.0       35317.0   \n",
       "11              Folsom      California       41051.0       35317.0   \n",
       "12        Philadelphia    Pennsylvania      741270.0      826172.0   \n",
       "13             Wichita          Kansas      192354.0      197601.0   \n",
       "14             Wichita          Kansas      192354.0      197601.0   \n",
       "15          Fort Myers         Florida       36850.0       37165.0   \n",
       "16          Pittsburgh    Pennsylvania      149690.0      154695.0   \n",
       "17              Laredo           Texas      124305.0      131484.0   \n",
       "18            Berkeley      California       60142.0       60829.0   \n",
       "19         Santa Clara      California       63278.0       62938.0   \n",
       "20               Allen    Pennsylvania       60626.0       59581.0   \n",
       "21             Hampton        Virginia       66214.0       70240.0   \n",
       "22         Bolingbrook        Illinois       36295.0       35801.0   \n",
       "23           Frederick        Maryland       33146.0       36336.0   \n",
       "24              Sparks          Nevada       47780.0       48318.0   \n",
       "25      Rancho Cordova      California       34844.0       36182.0   \n",
       "26         Westminster        Colorado       54866.0       58251.0   \n",
       "27            Lakewood        Colorado       76013.0       76576.0   \n",
       "28               Flint        Michigan       48984.0       49313.0   \n",
       "29           New Haven     Connecticut       63765.0       66545.0   \n",
       "...                ...             ...           ...           ...   \n",
       "2861          Scranton    Pennsylvania       37975.0       39137.0   \n",
       "2862            Aurora        Colorado      177899.0      180971.0   \n",
       "2863          Redlands      California       33993.0       37035.0   \n",
       "2864          Alhambra      California       42184.0       43388.0   \n",
       "2865          Alhambra      California       42184.0       43388.0   \n",
       "2866        Rio Rancho      New Mexico       47251.0       46904.0   \n",
       "2867           Wyoming        Michigan       35967.0       39310.0   \n",
       "2868           Reading    Pennsylvania       43668.0       44205.0   \n",
       "2869            Rialto      California       49902.0       53235.0   \n",
       "2870       Tallahassee         Florida       89390.0      100504.0   \n",
       "2871             Hemet      California       38622.0       45251.0   \n",
       "2872            Aurora        Illinois      101964.0      101751.0   \n",
       "2873            Carmel         Indiana       42928.0       46101.0   \n",
       "2874          Glendale      California       98181.0      102844.0   \n",
       "2875        Des Moines            Iowa      103726.0      106591.0   \n",
       "2876   College Station           Texas       54125.0       53774.0   \n",
       "2877            Peoria         Arizona       80139.0       91103.0   \n",
       "2878         Charlotte  North Carolina      396646.0      430475.0   \n",
       "2879         Lafayette         Indiana       34313.0       36857.0   \n",
       "2880           Abilene           Texas       65212.0       60664.0   \n",
       "2881          Gulfport     Mississippi       33108.0       38764.0   \n",
       "2882             Davis      California       33493.0       34163.0   \n",
       "2883       Los Angeles      California     1958998.0     2012898.0   \n",
       "2884      Mount Vernon        New York       31876.0       36745.0   \n",
       "2885         Lynchburg        Virginia       38614.0       41198.0   \n",
       "2886          Stockton      California      150976.0      154674.0   \n",
       "2887        Southfield        Michigan       31369.0       41808.0   \n",
       "2888      Indianapolis         Indiana      410615.0      437808.0   \n",
       "2889        Somerville   Massachusetts       41028.0       39306.0   \n",
       "2890     Coral Springs         Florida       63316.0       66186.0   \n",
       "\n",
       "      total_population  num_of_veterans  foreign_born state_code  \\\n",
       "0                82463           1562.0       30908.0         MD   \n",
       "1                93629           4147.0       32935.0         MA   \n",
       "2                84839           4819.0        8229.0         AL   \n",
       "3               175232           5821.0       33878.0         CA   \n",
       "4               281913           5829.0       86253.0         NJ   \n",
       "5               118661           6634.0        7517.0         IL   \n",
       "6                80683           4815.0        8355.0         AZ   \n",
       "7               108489           3800.0       37038.0         CA   \n",
       "8                85032           5783.0        3269.0         MO   \n",
       "9               109828           5204.0       16315.0         NC   \n",
       "10               76368           4187.0       13234.0         CA   \n",
       "11               76368           4187.0       13234.0         CA   \n",
       "12             1567442          61995.0      205339.0         PA   \n",
       "13              389955          23978.0       40270.0         KS   \n",
       "14              389955          23978.0       40270.0         KS   \n",
       "15               74015           4312.0       15365.0         FL   \n",
       "16              304385          17728.0       28187.0         PA   \n",
       "17              255789           4921.0       68427.0         TX   \n",
       "18              120971           3736.0       25000.0         CA   \n",
       "19              126216           4426.0       52281.0         CA   \n",
       "20              120207           5691.0       19652.0         PA   \n",
       "21              136454          19638.0        6204.0         VA   \n",
       "22               72096           2951.0       15212.0         IL   \n",
       "23               69482           3870.0       14211.0         MD   \n",
       "24               96098           7315.0       15690.0         NV   \n",
       "25               71026           4590.0       17020.0         CA   \n",
       "26              113117           6512.0       11361.0         CO   \n",
       "27              152589           9988.0       14169.0         CO   \n",
       "28               98297           3757.0        2138.0         MI   \n",
       "29              130310           2567.0       25871.0         CT   \n",
       "...                ...              ...           ...        ...   \n",
       "2861             77112           5059.0        8069.0         PA   \n",
       "2862            358870          25158.0       65816.0         CO   \n",
       "2863             71028           3445.0        8681.0         CA   \n",
       "2864             85572           1673.0       44441.0         CA   \n",
       "2865             85572           1673.0       44441.0         CA   \n",
       "2866             94155           8527.0        5200.0         NM   \n",
       "2867             75277           3447.0        8612.0         MI   \n",
       "2868             87873           2580.0       17043.0         PA   \n",
       "2869            103137           2036.0       32741.0         CA   \n",
       "2870            189894           9575.0       16720.0         FL   \n",
       "2871             83873           5613.0       13330.0         CA   \n",
       "2872            203715           4988.0       50980.0         IL   \n",
       "2873             89029           2723.0        9645.0         IN   \n",
       "2874            201025           4448.0      111510.0         CA   \n",
       "2875            210317          11780.0       23857.0         IA   \n",
       "2876            107899           2471.0       16145.0         TX   \n",
       "2877            171242          13019.0       14260.0         AZ   \n",
       "2878            827121          36046.0      128897.0         NC   \n",
       "2879             71170           5045.0        5697.0         IN   \n",
       "2880            125876           9367.0        8129.0         TX   \n",
       "2881             71872           6646.0        3072.0         MS   \n",
       "2882             67656           2176.0       13997.0         CA   \n",
       "2883           3971896          85417.0     1485425.0         CA   \n",
       "2884             68621           2064.0       23777.0         NY   \n",
       "2885             79812           4322.0        4364.0         VA   \n",
       "2886            305650          12822.0       79583.0         CA   \n",
       "2887             73177           4035.0        4011.0         MI   \n",
       "2888            848423          42186.0       72456.0         IN   \n",
       "2889             80334           2103.0       22292.0         MA   \n",
       "2890            129502           4724.0       38552.0         FL   \n",
       "\n",
       "                                   race  \n",
       "0                    Hispanic or Latino  \n",
       "1                                 White  \n",
       "2                                 Asian  \n",
       "3             Black or African-American  \n",
       "4                                 White  \n",
       "5     American Indian and Alaska Native  \n",
       "6             Black or African-American  \n",
       "7                                 Asian  \n",
       "8                    Hispanic or Latino  \n",
       "9                                 Asian  \n",
       "10                   Hispanic or Latino  \n",
       "11    American Indian and Alaska Native  \n",
       "12                                Asian  \n",
       "13                   Hispanic or Latino  \n",
       "14    American Indian and Alaska Native  \n",
       "15                                White  \n",
       "16                                White  \n",
       "17    American Indian and Alaska Native  \n",
       "18                                Asian  \n",
       "19                                White  \n",
       "20            Black or African-American  \n",
       "21            Black or African-American  \n",
       "22    American Indian and Alaska Native  \n",
       "23                                White  \n",
       "24                   Hispanic or Latino  \n",
       "25                                Asian  \n",
       "26                   Hispanic or Latino  \n",
       "27                   Hispanic or Latino  \n",
       "28                                White  \n",
       "29    American Indian and Alaska Native  \n",
       "...                                 ...  \n",
       "2861                              Asian  \n",
       "2862                              Asian  \n",
       "2863                 Hispanic or Latino  \n",
       "2864                 Hispanic or Latino  \n",
       "2865          Black or African-American  \n",
       "2866          Black or African-American  \n",
       "2867  American Indian and Alaska Native  \n",
       "2868                              Asian  \n",
       "2869  American Indian and Alaska Native  \n",
       "2870                              Asian  \n",
       "2871                              Asian  \n",
       "2872          Black or African-American  \n",
       "2873                              White  \n",
       "2874                 Hispanic or Latino  \n",
       "2875                              White  \n",
       "2876                              White  \n",
       "2877                              Asian  \n",
       "2878  American Indian and Alaska Native  \n",
       "2879                 Hispanic or Latino  \n",
       "2880          Black or African-American  \n",
       "2881                              White  \n",
       "2882  American Indian and Alaska Native  \n",
       "2883          Black or African-American  \n",
       "2884                 Hispanic or Latino  \n",
       "2885                              White  \n",
       "2886  American Indian and Alaska Native  \n",
       "2887  American Indian and Alaska Native  \n",
       "2888                              White  \n",
       "2889  American Indian and Alaska Native  \n",
       "2890                              White  \n",
       "\n",
       "[2889 rows x 9 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo_info.loc[df_demo_info.duplicated(subset=['city'],keep=False), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Dropping missing values and duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# immigration data\n",
    "fact_immigration.dropna(subset=['cic_id'])\n",
    "dim_immigration_personal.dropna(subset=['cic_id'])\n",
    "dim_immigration_air.dropna(subset=['cic_id'])\n",
    "\n",
    "fact_immigration.drop_duplicates(subset = 'cic_id', keep = 'first')\n",
    "dim_immigration_personal.drop_duplicates(subset = 'cic_id', keep = 'first')\n",
    "dim_immigration_air.drop_duplicates(subset = 'cic_id', keep = 'first')\n",
    "\n",
    "# temperature data\n",
    "df_temp_us.dropna()\n",
    "df_temp_us.drop_duplicates(subset = 'dt', keep = 'first')\n",
    "\n",
    "# demography data\n",
    "df_demo_info.dropna()\n",
    "df_demo_stat.dropna()\n",
    "df_demo_info.drop_duplicates(subset = 'city', keep = 'first', inplace=True)\n",
    "df_demo_stat.drop_duplicates(subset = 'city', keep = 'first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "<img width=\"802\" alt=\"Screen Shot 2021-09-09 at 10 46 07 PM\" src=\"https://user-images.githubusercontent.com/79597984/132707777-a124e5d3-03d0-45c6-9bd5-61a8fc6f5618.png\">\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "1. Create the data model by copying the drop and create statements from `create_tables.sql` file into the query editor in redshift.\n",
    "2. Run step 1 & 2 above to clean the data.\n",
    "3. Insert the data into the tables created by running the below code in step `4.1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here\n",
    "conn = psycopg2.connect(\"host=redshift-cluster-1.********.us-west-2.redshift.amazonaws.com dbname=dev user=awsuser password=******* port=5439\")\n",
    "cur = conn.cursor()\n",
    "conn.autocommit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index, row in fact_immigration.iterrows():\n",
    "    cur.execute(insert_fact, list(row.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index, row in dim_immigration_personal.iterrows():\n",
    "    cur.execute(insert_dim_imm_per, list(row.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index, row in dim_immigration_air.iterrows():\n",
    "    cur.execute(insert_dim_imm_air, list(row.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_demo_info.iterrows():\n",
    "    cur.execute(insert_dim_demo_info, list(row.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_demo_stat.iterrows():\n",
    "    cur.execute(insert_dim_demo_stat, list(row.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_temp_us.iterrows():\n",
    "    cur.execute(insert_dim_temp, list(row.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Checking if the duplicated rows are all removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "There's no duplicated rows\n"
     ]
    }
   ],
   "source": [
    "duplicate_query = \"\"\"\n",
    "    SELECT cic_id, COUNT(cic_id)\n",
    "    FROM fact_immigration\n",
    "    GROUP BY cic_id\n",
    "    HAVING COUNT(cic_id)>1;\n",
    "\"\"\"\n",
    "cur.execute(duplicate_query)\n",
    "data = cur.fetchall()\n",
    "\n",
    "if not data:\n",
    "    print(\"There's no duplicated rows\")\n",
    "else:\n",
    "    print(f\"There's {len(data)} duplicated rows in cic_id in the table fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "There's no duplicated rows\n"
     ]
    }
   ],
   "source": [
    "duplicate_query = \"\"\"\n",
    "    SELECT cic_id, COUNT(cic_id)\n",
    "    FROM dim_immigration_personal\n",
    "    GROUP BY cic_id\n",
    "    HAVING COUNT(cic_id)>1;\n",
    "\"\"\"\n",
    "cur.execute(duplicate_query)\n",
    "data = cur.fetchall()\n",
    "\n",
    "if not data:\n",
    "    print(\"There's no duplicated rows\")\n",
    "else:\n",
    "    print(f\"There's {len(data)} duplicated rows in cic_id in the table dim_immigration_personal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "There's no duplicated rows\n"
     ]
    }
   ],
   "source": [
    "duplicate_query = \"\"\"\n",
    "    SELECT cic_id, COUNT(cic_id)\n",
    "    FROM dim_immigration_air\n",
    "    GROUP BY cic_id\n",
    "    HAVING COUNT(cic_id)>1;\n",
    "\"\"\"\n",
    "cur.execute(duplicate_query)\n",
    "data = cur.fetchall()\n",
    "\n",
    "if not data:\n",
    "    print(\"There's no duplicated rows\")\n",
    "else:\n",
    "    print(f\"There's {len(data)} duplicated rows in cic_id in the table dim_immigration_air\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "There's no duplicated rows\n"
     ]
    }
   ],
   "source": [
    "duplicate_query = \"\"\"\n",
    "    SELECT city, COUNT(city)\n",
    "    FROM dim_demo_info\n",
    "    GROUP BY city\n",
    "    HAVING COUNT(city)>1;\n",
    "\"\"\"\n",
    "cur.execute(duplicate_query)\n",
    "data = cur.fetchall()\n",
    "\n",
    "if not data:\n",
    "    print(\"There's no duplicated rows\")\n",
    "else:\n",
    "    print(f\"There's {len(data)} duplicated rows in city in the table dim_demo_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "There's no duplicated rows\n"
     ]
    }
   ],
   "source": [
    "duplicate_query = \"\"\"\n",
    "    SELECT city, COUNT(city)\n",
    "    FROM dim_demo_stat\n",
    "    GROUP BY city\n",
    "    HAVING COUNT(city)>1;\n",
    "\"\"\"\n",
    "cur.execute(duplicate_query)\n",
    "data = cur.fetchall()\n",
    "\n",
    "if not data:\n",
    "    print(\"There's no duplicated rows\")\n",
    "else:\n",
    "    print(f\"There's {len(data)} duplicated rows in city in the table dim_demo_stat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "There's no duplicated rows\n"
     ]
    }
   ],
   "source": [
    "duplicate_query = \"\"\"\n",
    "    SELECT dt, COUNT(dt)\n",
    "    FROM dim_temp\n",
    "    GROUP BY dt\n",
    "    HAVING COUNT(dt)>1;\n",
    "\"\"\"\n",
    "cur.execute(duplicate_query)\n",
    "data = cur.fetchall()\n",
    "\n",
    "if not data:\n",
    "    print(\"There's no duplicated rows\")\n",
    "else:\n",
    "    print(f\"There's {len(data)} duplicated rows in dt in the table dim_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Checking if the rows exist after inserting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2000,)]\n",
      "Successfully executed with 2000 records in the table fact_immigration!\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT COUNT(*) FROM fact_immigration;\"\n",
    "cur.execute(query) \n",
    "data = cur.fetchall()\n",
    "\n",
    "if data[0][0] < 1:\n",
    "    print(\"No data found in table fact_immigration\")\n",
    "else:\n",
    "    print(f\"Successfully executed with {data[0][0]} records in the table fact_immigration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2000,)]\n",
      "Successfully executed with 2000 records in the table dim_immigration_personal!!\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT COUNT(*) FROM dim_immigration_personal;\"\n",
    "cur.execute(query) \n",
    "data = cur.fetchall()\n",
    "print(data)\n",
    "\n",
    "if data[0][0] < 1:\n",
    "    print(\"No data found in table dim_immigration_personal\")\n",
    "else:\n",
    "    print(f\"Successfully executed with {data[0][0]} records in the table dim_immigration_personal!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2000,)]\n",
      "Successfully executed with 2000 records in the table dim_immigration_air!!\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT COUNT(*) FROM dim_immigration_air;\"\n",
    "cur.execute(query) \n",
    "data = cur.fetchall()\n",
    "\n",
    "if data[0][0] < 1:\n",
    "    print(\"No data found in table dim_immigration_air\")\n",
    "else:\n",
    "    print(f\"Successfully executed with {data[0][0]} records in the table dim_immigration_air!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(567,)]\n",
      "Successfully executed with 567 records in the table dim_demo_info!!\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT COUNT(*) FROM dim_demo_info;\"\n",
    "cur.execute(query) \n",
    "data = cur.fetchall()\n",
    "\n",
    "if data[0][0] < 1:\n",
    "    print(\"No data found in table dim_demo_info\")\n",
    "else:\n",
    "    print(f\"Successfully executed with {data[0][0]} records in the table dim_demo_info!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(567,)]\n",
      "Successfully executed with 567 records in the table dim_demo_stat!!\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT COUNT(*) FROM dim_demo_stat;\"\n",
    "cur.execute(query) \n",
    "data = cur.fetchall()\n",
    "\n",
    "if data[0][0] < 1:\n",
    "    print(\"No data found in table dim_demo_stat\")\n",
    "else:\n",
    "    print(f\"Successfully executed with {data[0][0]} records in the table dim_demo_stat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2000,)]\n",
      "Successfully executed with 2000 records in the table dim_temp!!\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT COUNT(*) FROM dim_temp;\"\n",
    "cur.execute(query) \n",
    "data = cur.fetchall()\n",
    "\n",
    "if data[0][0] < 1:\n",
    "    print(\"No data found in table dim_temp\")\n",
    "else:\n",
    "    print(f\"Successfully executed with {data[0][0]} records in the table dim_temp!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Sources:\n",
    "- Immigration dataset : [US NTTO website](https://travel.trade.gov/research/reports/i94/historical/2016.html)\n",
    "- Temperature dataset : [Kaggle](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)\n",
    "- Demographic dataset : [OpenSoft](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "\n",
    "__Fact Table__\n",
    "- fact_immigration<br>\n",
    "`cic_id` : city id<br>\n",
    "`year` : year the object came in<br>\n",
    "`month` : month the object came in<br>\n",
    "`dep_city` : departing city<br>\n",
    "`arrival_date` : arrival date in the US<br>\n",
    "`dep_date` : departure date from the US<br>\n",
    "`travel_code` : travel code<br>\n",
    "`visa` : visa codes in 3 categories<br>\n",
    "\n",
    "__Dimension Tables__\n",
    "- dim_immigration_personal<br>\n",
    "`cic_id` : city id<br>\n",
    "`citizen_country` : the city the object is citizen in<br>\n",
    "`resident_country` : residing city<br>\n",
    "`birthyear` : birthyear in 4 digits<br>\n",
    "`gender` : gender<br>\n",
    "`ins_number` : INS number<br>\n",
    "<br>\n",
    "- dim_immigration_air<br>\n",
    "`cic_id` : city id<br>\n",
    "`airline` : airline used to arrive in the US<br>\n",
    "`admin_number` : admission number<br>\n",
    "`flight_number` : flight number of Airline used to arrive in the US<br>\n",
    "`visa` : visa codes in 3 categories<br>\n",
    "`visa_type` : class of admission legally admitting the non-immigrant to temporarily stay in the US<br>\n",
    "<br>\n",
    "- dim_temp<br>\n",
    "`dt` : date<br>\n",
    "`avg_temp` : average temperature<br>\n",
    "`avg_temp_uncertainty` : average tempertaure uncertainty<br>\n",
    "`city` : city <br>\n",
    "`country` : country<br>\n",
    "`latitude` : latitude<br>\n",
    "`longitude` : longitude<br>\n",
    "<br>\n",
    "- dim_demo_info<br>\n",
    "`city` : city in the US<br>\n",
    "`state` : state in the US<br>\n",
    "`m_population` : male population of the city<br>\n",
    "`f_population` : female population of the city<br>\n",
    "`total_population` : total population of the city<br>\n",
    "`num_of_veterans` : number of veterans in the city<br>\n",
    "`foreign_born` : number of foreign-borns in. the city <br>\n",
    "`state_code` : state code<br>\n",
    "`race` : race of the object<br>\n",
    "<br>\n",
    "- dim_demo_stat<br>\n",
    "`city` : city in the US<br>\n",
    "`state` : state in the US<br>\n",
    "`median_age` : median age of the population in the city<br>\n",
    "`avg_household_size` : average household size of the population of the city<br> \n",
    "`state_code` : state code<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "##### Steps taken in the project\n",
    "1. Load the data using Pandas DataFrame and then process them\n",
    "2. Clean the processed data frames\n",
    "    - Remove the rows with unmatching data types\n",
    "    - Remove empty rows\n",
    "    - Remove duplicates\n",
    "    - Convert the column to a better data type\n",
    "3. Create the tables using SQL and Redshift, and also create the schema\n",
    "4. Connect to Redshift cluster and insert the data we prepared in step 1 & 2\n",
    "5. Go through Data qulity check in order to see if the tables were made in the way we wanted\n",
    "    - Check if there's any duplicated data\n",
    "    - Check if any of the rows are empty\n",
    "\n",
    "##### Purpose of the output model\n",
    "The output data model will be used for further analysis of the information we obtained from the above steps. For example, we would be able to find out the relationship between the temperature and the population of the city in the US.\n",
    "\n",
    "##### Choice of tools\n",
    "1. Pandas DataFrame to process and clean the data before inserting.\n",
    "2. Spark(PySpark) to insert the data into the schema created with SQL.\n",
    "\n",
    "##### How often the data should be updated and why\n",
    "The immigration data should be updated monthly since it's in a monthly form, and the temperature data should be updated annually.\n",
    "\n",
    "##### Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " - Using `Spark` along with `EMR(Hadoop)` should help me process the data in much bigger size. This way, I would be able to handle the copy of the data on cloud.\n",
    " \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " - For the automated system like this situation, `Apache Airflow` would be the best choice since we can schedule the ETL process within Airflow. With this powerful automation tool, we can also send out emails to other teams in charge when there's an error in the task.\n",
    " \n",
    " * The database needed to be accessed by 100+ people.\n",
    " - `AWS Redshift` will allow us to have out database accessed by a number of people in a stable manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
